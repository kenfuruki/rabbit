# 第一ステージ　応用数学レポート
## 第１章：線形代数
* 行列という発想  
行列は、連立方程式を解くという発想からきた。解を求めようとしたときに、行基本形を繰り返していく過程で、  
逆数のような行列「逆行列」が出てきた。  
逆行列の求め方はガウスの吐き出し法で算出する。
  
* 行列式  
行列には必ず逆行列が存在するわけではない。行列式は2行2列の行列を例にとると、  
２つのベクトルを辺にもつ平行四辺形の面積にあたるイメージで、スカラーである。  
この行列式の値がゼロになる場合は、逆行列が存在しない。
  
* 固有値、固有ベクトル  
固有ベクトルは、上式を満たすｘのベクトルで、ゼロベクトルではないもの。  
固有値は、Aの固有値として上式のλを満たすスカラー値が固有値。  
固有値、固有ベクトルは正方行列のみであり、それに対し長方行列は、特異値、特異ベクトルが似た考えである。

$$
A \boldsymbol{x} \quad = \quad  \lambda \boldsymbol{x}
$$

## 第２章：確率・統計
* 確率変数  
事象と結びつけられた数値である。  
* 期待値  
要素ごとに確率変数に確率をかけたものの合算である。連続する値であるなら、確率変数と確率の積分になる。

* 分散、共分散  
分散は、値のちらばり方を示しており、確率変数と期待値（確率変数の平均）の差を二乗したもの。  
分解すると、二乗の平均に平均の二乗の差をとる。  
共分散は、２つのデータ系列の傾向の違いを表すもので、無関係だとゼロに近づく。

* 標準偏差  
　分散のままだと二乗しているため、単位の次元が異なってしまうため、ルートを取った値が標準偏差

## 第３章：情報理論
* 自己情報量  
同じ増分1に対して、10に対する増分なのか、1000に増分なのかで、そのめずらしさに違いがある。  
それを自己情報量といい、$$I（x）= -log⁡(P(x)) $$であらわされる。　
* 交差エントロピー  
確率分布Qについての自己情報量をPの分布で平均しているもの。  
つまり、PとQがどれだけ近いかを表現する関数。  
機械学習の分類問題に対する損失関数に使用されることが多い。  
分類問題の場合は、教師データがゼロかイチのみになる二択になるため、  
シグマで合計値をとってもゼロの分は足されず、一項のみとなるため、シグマがなくなる以下の式になる。$$ E = -log(p(k))$$

