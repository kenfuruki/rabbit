{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"3_1_simple_RNN_after_kenfuruki.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"8cNl2QA_Rnv5"},"source":["# 準備"]},{"cell_type":"markdown","metadata":{"id":"YkwjN1jNVAYy"},"source":["## Googleドライブのマウント"]},{"cell_type":"code","metadata":{"id":"pvFXpiH3EVC1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637383048340,"user_tz":-540,"elapsed":292,"user":{"displayName":"naoken furufuru","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11277346411526010614"}},"outputId":"bb77af02-9615-4a7f-85a1-a2877b4a0212"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"3Ub7RYdeY6pK"},"source":["## sys.pathの設定"]},{"cell_type":"markdown","metadata":{"id":"oql7L19rEsWi"},"source":["以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"]},{"cell_type":"code","metadata":{"id":"7Ic2JzkvFX59","executionInfo":{"status":"ok","timestamp":1637383056987,"user_tz":-540,"elapsed":274,"user":{"displayName":"naoken furufuru","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11277346411526010614"}}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/E_DEEP_day3_4')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzGmsHRwO-bi"},"source":["# simple RNN after\n","### バイナリ加算"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"KNSG0aKXO-bk","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1637383798317,"user_tz":-540,"elapsed":10208,"user":{"displayName":"naoken furufuru","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11277346411526010614"}},"outputId":"8502c3fa-ec41-4221-c676-a95da648d0b5"},"source":["import numpy as np\n","from common import functions\n","import matplotlib.pyplot as plt\n","\n","\n","def d_tanh(x):\n","    return 1/(np.cosh(x) ** 2)\n","\n","# データを用意\n","# 2進数の桁数\n","binary_dim = 8\n","# 最大値 + 1\n","largest_number = pow(2, binary_dim)\n","# largest_numberまで2進数を用意\n","binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n","\n","input_layer_size = 2\n","hidden_layer_size = 16\n","output_layer_size = 1\n","\n","weight_init_std = 1\n","learning_rate = 0.1\n","\n","iters_num = 10000\n","plot_interval = 100\n","\n","# ウェイト初期化 (バイアスは簡単のため省略)\n","W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n","W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n","W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n","# Xavier\n","# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n","# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n","# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n","# He\n","# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n","# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n","# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n","\n","\n","# 勾配\n","W_in_grad = np.zeros_like(W_in)\n","W_out_grad = np.zeros_like(W_out)\n","W_grad = np.zeros_like(W)\n","\n","u = np.zeros((hidden_layer_size, binary_dim + 1))\n","z = np.zeros((hidden_layer_size, binary_dim + 1))\n","y = np.zeros((output_layer_size, binary_dim))\n","\n","delta_out = np.zeros((output_layer_size, binary_dim))\n","delta = np.zeros((hidden_layer_size, binary_dim + 1))\n","\n","all_losses = []\n","\n","for i in range(iters_num):\n","    \n","    # A, B初期化 (a + b = d)\n","    a_int = np.random.randint(largest_number/2)\n","    a_bin = binary[a_int] # binary encoding\n","    b_int = np.random.randint(largest_number/2)\n","    b_bin = binary[b_int] # binary encoding\n","    \n","    # 正解データ\n","    d_int = a_int + b_int\n","    d_bin = binary[d_int]\n","    \n","    # 出力バイナリ\n","    out_bin = np.zeros_like(d_bin)\n","    \n","    # 時系列全体の誤差\n","    all_loss = 0    \n","    \n","    # 時系列ループ\n","    for t in range(binary_dim):\n","        # 入力値\n","        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n","        # 時刻tにおける正解データ\n","        dd = np.array([d_bin[binary_dim - t - 1]])\n","        \n","        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n","#        z[:,t+1] = functions.sigmoid(u[:,t+1])\n","        z[:,t+1] = functions.relu(u[:,t+1])\n","#        z[:,t+1] = np.tanh(u[:,t+1])    \n","        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n","\n","\n","        #誤差\n","        loss = functions.mean_squared_error(dd, y[:,t])\n","        \n","        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n","        \n","        all_loss += loss\n","\n","        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n","    \n","    \n","    for t in range(binary_dim)[::-1]:\n","        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n","\n","#        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n","        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n","#        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n","\n","        # 勾配更新\n","        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n","        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n","        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n","    \n","    # 勾配適用\n","    W_in -= learning_rate * W_in_grad\n","    W_out -= learning_rate * W_out_grad\n","    W -= learning_rate * W_grad\n","    \n","    W_in_grad *= 0\n","    W_out_grad *= 0\n","    W_grad *= 0\n","    \n","\n","    if(i % plot_interval == 0):\n","        all_losses.append(all_loss)        \n","        print(\"iters:\" + str(i))\n","        print(\"Loss:\" + str(all_loss))\n","        print(\"Pred:\" + str(out_bin))\n","        print(\"True:\" + str(d_bin))\n","        out_int = 0\n","        for index,x in enumerate(reversed(out_bin)):\n","            out_int += x * pow(2, index)\n","        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n","        print(\"------------\")\n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, all_losses, label=\"loss\")\n","plt.show()"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["iters:0\n","Loss:3.318509414009432\n","Pred:[0 1 0 1 1 1 1 1]\n","True:[1 0 1 1 0 0 0 0]\n","83 + 93 = 95\n","------------\n","iters:100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 1 0 1 0]\n","122 + 96 = 0\n","------------\n","iters:200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/My Drive/E_DEEP_day3_4/common/functions.py:6: RuntimeWarning: overflow encountered in exp\n","  return 1/(1 + np.exp(-x))\n"]},{"output_type":"stream","name":"stdout","text":["True:[1 0 0 0 1 0 1 0]\n","127 + 11 = 0\n","------------\n","iters:300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 1 0 0 1 1 0]\n","126 + 104 = 0\n","------------\n","iters:400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 0 0 1]\n","26 + 103 = 0\n","------------\n","iters:500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 0 1 1 0 0 1 1]\n","45 + 6 = 0\n","------------\n","iters:600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 0 0 1]\n","94 + 35 = 0\n","------------\n","iters:700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 0 0 0 0 1]\n","95 + 98 = 0\n","------------\n","iters:800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 0 0 0 0]\n","82 + 126 = 0\n","------------\n","iters:900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 1 0 0 0]\n","96 + 120 = 0\n","------------\n","iters:1000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 1 0 1 1]\n","116 + 103 = 0\n","------------\n","iters:1100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 0 1 1]\n","82 + 25 = 0\n","------------\n","iters:1200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 0 1 1 1]\n","59 + 12 = 0\n","------------\n","iters:1300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 1 1 1]\n","28 + 107 = 0\n","------------\n","iters:1400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 1 0 1]\n","59 + 50 = 0\n","------------\n","iters:1500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 1 1 1]\n","10 + 125 = 0\n","------------\n","iters:1600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 1 0 0 1 0 0]\n","113 + 115 = 0\n","------------\n","iters:1700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 1 0 1 1 1]\n","29 + 122 = 0\n","------------\n","iters:1800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 1 1 0 1]\n","32 + 93 = 0\n","------------\n","iters:1900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 0 1 1]\n","98 + 9 = 0\n","------------\n","iters:2000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 0 1 0 0 0 0 0]\n","18 + 14 = 0\n","------------\n","iters:2100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 1 0 0]\n","20 + 88 = 0\n","------------\n","iters:2200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 0 1 1]\n","61 + 46 = 0\n","------------\n","iters:2300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 1 1 1 1 0]\n","101 + 57 = 0\n","------------\n","iters:2400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 1 0 0 0]\n","15 + 105 = 0\n","------------\n","iters:2500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 1 0 0 0 0 1]\n","126 + 99 = 0\n","------------\n","iters:2600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 0 1 1 0 0]\n","122 + 82 = 0\n","------------\n","iters:2700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 0 0 1 1]\n","58 + 41 = 0\n","------------\n","iters:2800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 0 1 0 1]\n","47 + 38 = 0\n","------------\n","iters:2900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 1 0 0]\n","84 + 48 = 0\n","------------\n","iters:3000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 1 1 0 1 0 1]\n","67 + 114 = 0\n","------------\n","iters:3100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 0 1 1 1 1 0 0]\n","17 + 43 = 0\n","------------\n","iters:3200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 0 1 1 0]\n","121 + 93 = 0\n","------------\n","iters:3300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 1 1 1 1]\n","57 + 70 = 0\n","------------\n","iters:3400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 0 1 1 0 1]\n","84 + 121 = 0\n","------------\n","iters:3500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 0 0 1 0]\n","49 + 17 = 0\n","------------\n","iters:3600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 0 0 1 0]\n","10 + 72 = 0\n","------------\n","iters:3700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 0 0 0 1 1 0 1]\n","11 + 2 = 0\n","------------\n","iters:3800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 0 1 0 0 1]\n","108 + 93 = 0\n","------------\n","iters:3900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 0 0 1 0]\n","15 + 51 = 0\n","------------\n","iters:4000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 1 1 1 0]\n","112 + 14 = 0\n","------------\n","iters:4100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 1 1 1 0 1]\n","70 + 87 = 0\n","------------\n","iters:4200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 0 0 1]\n","13 + 92 = 0\n","------------\n","iters:4300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 1 1 0 1]\n","103 + 118 = 0\n","------------\n","iters:4400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 0 1 0 0]\n","64 + 20 = 0\n","------------\n","iters:4500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 1 0 0]\n","103 + 5 = 0\n","------------\n","iters:4600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 0 1 1 1 0 1 1]\n","50 + 9 = 0\n","------------\n","iters:4700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 1 1 0 0]\n","60 + 80 = 0\n","------------\n","iters:4800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 1 1 1 0]\n","78 + 0 = 0\n","------------\n","iters:4900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 0 0 1 1]\n","49 + 66 = 0\n","------------\n","iters:5000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 0 1 1 0]\n","61 + 25 = 0\n","------------\n","iters:5100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 1 1 1 0]\n","24 + 70 = 0\n","------------\n","iters:5200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 0 0 0]\n","2 + 126 = 0\n","------------\n","iters:5300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 1 1 1 1 1 0]\n","86 + 104 = 0\n","------------\n","iters:5400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 1 1 0]\n","96 + 14 = 0\n","------------\n","iters:5500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 1 1 0 0 1 0]\n","86 + 92 = 0\n","------------\n","iters:5600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 1 0 1 1]\n","27 + 96 = 0\n","------------\n","iters:5700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 0 0 0 0]\n","47 + 17 = 0\n","------------\n","iters:5800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 1 1 0 1]\n","97 + 124 = 0\n","------------\n","iters:5900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 0 0 1 0]\n","127 + 83 = 0\n","------------\n","iters:6000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 0 1 1 0 1]\n","100 + 105 = 0\n","------------\n","iters:6100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 0 0 0 1]\n","65 + 16 = 0\n","------------\n","iters:6200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 1 1 1 1 0 0]\n","114 + 74 = 0\n","------------\n","iters:6300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 1 1 1 0 1]\n","119 + 102 = 0\n","------------\n","iters:6400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 1 0 1 0]\n","48 + 26 = 0\n","------------\n","iters:6500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 1 1 0 0]\n","101 + 39 = 0\n","------------\n","iters:6600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 0 1 1]\n","44 + 87 = 0\n","------------\n","iters:6700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 1 0 0]\n","27 + 105 = 0\n","------------\n","iters:6800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 0 0 0 1]\n","48 + 65 = 0\n","------------\n","iters:6900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 0 0 1 1 1 0 0]\n","18 + 10 = 0\n","------------\n","iters:7000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 1 0 1 1]\n","85 + 54 = 0\n","------------\n","iters:7100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 1 0 1 1]\n","22 + 101 = 0\n","------------\n","iters:7200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 1 0 0 1]\n","35 + 54 = 0\n","------------\n","iters:7300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 1 0 0]\n","32 + 76 = 0\n","------------\n","iters:7400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 1 0 0 0 0 0 1]\n","97 + 96 = 0\n","------------\n","iters:7500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 0 0 0 1]\n","17 + 96 = 0\n","------------\n","iters:7600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 1 0 1 1 1]\n","71 + 80 = 0\n","------------\n","iters:7700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 1 0 0]\n","98 + 34 = 0\n","------------\n","iters:7800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 0 1 1 0]\n","82 + 20 = 0\n","------------\n","iters:7900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 1 0 0 1]\n","95 + 42 = 0\n","------------\n","iters:8000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 1 1 0 1]\n","86 + 23 = 0\n","------------\n","iters:8100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 0 0 1 0]\n","23 + 59 = 0\n","------------\n","iters:8200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 1 1 1 0]\n","19 + 59 = 0\n","------------\n","iters:8300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 0 1 0 1]\n","43 + 26 = 0\n","------------\n","iters:8400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 1 0 0 1 0]\n","35 + 111 = 0\n","------------\n","iters:8500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 0 0 0 0]\n","21 + 107 = 0\n","------------\n","iters:8600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 0 1 1 0 1 1 0]\n","23 + 31 = 0\n","------------\n","iters:8700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 1 0 1 0 0]\n","86 + 62 = 0\n","------------\n","iters:8800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 0 0 0 0 1]\n","6 + 59 = 0\n","------------\n","iters:8900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 0 1 0 1]\n","8 + 93 = 0\n","------------\n","iters:9000\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 0 0 1 0]\n","56 + 26 = 0\n","------------\n","iters:9100\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 1 1 1 0 0]\n","114 + 42 = 0\n","------------\n","iters:9200\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 0 0 1 0 1 0 0]\n","16 + 4 = 0\n","------------\n","iters:9300\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 0 0 1 0 1 1]\n","121 + 18 = 0\n","------------\n","iters:9400\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[1 0 1 1 1 1 0 0]\n","104 + 84 = 0\n","------------\n","iters:9500\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 0 1 0 0 0 1]\n","9 + 72 = 0\n","------------\n","iters:9600\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 0 1 0 1]\n","17 + 100 = 0\n","------------\n","iters:9700\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 0 1 1 0]\n","18 + 84 = 0\n","------------\n","iters:9800\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 0 0 1 1 1]\n","43 + 60 = 0\n","------------\n","iters:9900\n","Loss:1.0\n","Pred:[0 0 0 0 0 0 0 0]\n","True:[0 1 1 1 1 0 1 0]\n","15 + 107 = 0\n","------------\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"dsBt7vxmO-bn"},"source":["---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n","\n","\n","## [try] 重みの初期化方法を変更してみよう\n","Xavier, He\n","\n","## [try] 中間層の活性化関数を変更してみよう\n","ReLU(勾配爆発を確認しよう)<br>\n","tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n","\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"]}]}